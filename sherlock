#!/usr/bin/env bash
set -e

# Phase 5: Check for history command first
if [ "$1" = "history" ]; then
    shift  # Remove 'history' from args
    
    # Parse filters
    SERVICE_FILTER=""
    CATEGORY_FILTER=""
    DECISION_FILTER=""
    CONFIDENCE_FILTER=""
    SIGNAL_FILTER=""
    SHOW_CALIBRATION=false
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --service)
                SERVICE_FILTER="$2"
                shift 2
                ;;
            --category)
                CATEGORY_FILTER="$2"
                shift 2
                ;;
            --decision)
                DECISION_FILTER="$2"
                shift 2
                ;;
            --confidence-below)
                CONFIDENCE_FILTER="$2"
                shift 2
                ;;
            --signal)
                SIGNAL_FILTER="$2"
                shift 2
                ;;
            --calibration)
                SHOW_CALIBRATION=true
                shift
                ;;
            *)
                echo "Unknown option: $1"
                exit 1
                ;;
        esac
    done
    
    # Export filters for Python script
    export SERVICE_FILTER
    export CATEGORY_FILTER
    export DECISION_FILTER
    export CONFIDENCE_FILTER
    export SIGNAL_FILTER
    export SHOW_CALIBRATION
    
    # Run history query
    python3 - <<'HISTORY_QUERY'
import os
import sys
import json
import re
from pathlib import Path

data_dir = Path("incidents")
if not data_dir.exists():
    print("No incident history found (incidents/ does not exist)")
    sys.exit(0)

# Simple YAML parser for our specific format
def parse_incident_yaml(file_path):
    """Parse incident YAML into dict"""
    with open(file_path, 'r') as f:
        lines = f.readlines()
    
    incident = {}
    current_key = None
    current_list = None
    
    for line in lines:
        line_stripped = line.strip()
        if not line_stripped or line_stripped.startswith('#'):
            continue
        
        # Top-level key: value
        if not line.startswith(' ') and ':' in line:
            key, value = line.split(':', 1)
            key = key.strip()
            value = value.strip().strip('"')
            
            if value:
                # Try to convert to int if possible
                try:
                    if value.isdigit() or (value.startswith('+') or value.startswith('-')):
                        incident[key] = int(value)
                    else:
                        incident[key] = value
                except:
                    incident[key] = value
            else:
                current_key = key
                incident[key] = []
                current_list = current_key
        
        # List items (starts with -)
        elif line.startswith('  -'):
            value = line.strip()[1:].strip().strip('"')
            if current_list and current_list in incident:
                incident[current_list].append(value)
    
    return incident

# Load filters from environment
service_filter = os.getenv("SERVICE_FILTER", "")
category_filter = os.getenv("CATEGORY_FILTER", "")
decision_filter = os.getenv("DECISION_FILTER", "")
confidence_filter = os.getenv("CONFIDENCE_FILTER", "")
signal_filter = os.getenv("SIGNAL_FILTER", "")
show_calibration = os.getenv("SHOW_CALIBRATION", "false") == "true"

# Load all incident records
incidents = []
for yaml_file in sorted(data_dir.glob("*.yaml")):
    try:
        record = parse_incident_yaml(yaml_file)
        incidents.append(record)
    except Exception as e:
        print(f"âš ï¸  Skipping corrupt file {yaml_file.name}: {e}", file=sys.stderr)

if not incidents:
    print("No incidents found in history")
    sys.exit(0)

# Apply filters
filtered = incidents

if service_filter:
    filtered = [i for i in filtered if i.get('service') == service_filter]

if category_filter:
    filtered = [i for i in filtered if i.get('category') == category_filter]

if decision_filter:
    filtered = [i for i in filtered if i.get('decision') == decision_filter]

if confidence_filter:
    threshold = int(confidence_filter)
    filtered = [i for i in filtered if int(i.get('human_confidence', 100)) < threshold]

if signal_filter:
    filtered = [i for i in filtered if signal_filter in i.get('signals', [])]

# Display results
if show_calibration:
    # Confidence calibration analysis
    ai_confidences = []
    human_confidences = []
    deltas = []
    
    for i in incidents:
        try:
            ai_conf = int(i.get('ai_confidence', 0))
            human_conf = int(i.get('human_confidence', 0))
            delta = int(i.get('confidence_delta', 0))
            ai_confidences.append(ai_conf)
            human_confidences.append(human_conf)
            deltas.append(delta)
        except:
            pass
    
    if ai_confidences:
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("Confidence Calibration Analysis")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print(f"AI confidence average:     {sum(ai_confidences)/len(ai_confidences):.1f}%")
        print(f"Human confidence average:  {sum(human_confidences)/len(human_confidences):.1f}%")
        print(f"Mean delta:                {sum(deltas)/len(deltas):+.1f}%")
        print(f"Total incidents:           {len(incidents)}")
        print()
        
        # Distribution
        mean_delta = sum(deltas) / len(deltas)
        if mean_delta > 5:
            print("âš ï¸  AI systematically overconfident")
        elif mean_delta < -5:
            print("âš ï¸  AI systematically underconfident")
        else:
            print("âœ“ AI confidence well-calibrated")
    else:
        print("No calibration data available")
else:
    # Standard history display
    if not filtered:
        print("No incidents match filters")
        sys.exit(0)
    
    print(f"{'ID':<12} | {'Service':<15} | {'Category':<12} | {'Conf':<4} | {'Decision':<10} | {'Date':<10}")
    print("â”€" * 85)
    
    for incident in filtered:
        inc_id = incident.get('incident_id', 'unknown')
        service = incident.get('service', 'unknown')[:15]
        category = incident.get('category', 'unknown')[:12]
        confidence = incident.get('human_confidence', '0')
        decision = incident.get('decision', 'unknown')[:10]
        timestamp = incident.get('date', '')[:10]
        
        print(f"{inc_id:<12} | {service:<15} | {category:<12} | {confidence:<4}% | {decision:<10} | {timestamp:<10}")
    
    print()
    print(f"Total: {len(filtered)} incident(s)")
    
    # Signal pattern if requested
    if signal_filter:
        print()
        print(f"Incidents with signal '{signal_filter}':")
        for incident in filtered:
            cause = incident.get('primary_root_cause', 'unknown')
            print(f"  â€¢ {incident.get('incident_id', 'unknown')}: {cause}")
HISTORY_QUERY
    
    exit 0
fi

INCIDENT_ID="INC-123"
SCOPE_FILE="incident-scope.json"

echo "â„¹ï¸  Demo mode: using simulated incident data (INC-123)"
echo
echo "ğŸ” Sherlock investigating incident $INCIDENT_ID"
echo

# Validate scope file exists
if [[ ! -f "$SCOPE_FILE" ]]; then
    echo "âŒ Missing incident scope file: $SCOPE_FILE. Aborting."
    exit 1
fi

echo "ğŸ“‹ Loading incident scope from $SCOPE_FILE"

# Extract metadata from scope
SERVICE=$(python3 -c "import json; print(json.load(open('$SCOPE_FILE'))['service'])")
START_TIME=$(python3 -c "import json; print(json.load(open('$SCOPE_FILE'))['time_window']['start'])")
END_TIME=$(python3 -c "import json; print(json.load(open('$SCOPE_FILE'))['time_window']['end'])")
ENVIRONMENT="demo"
TIMEZONE="UTC"

if [[ -z "$SERVICE" || -z "$START_TIME" || -z "$END_TIME" ]]; then
    echo "âŒ Missing required scope fields (service, time_window). Aborting."
    exit 1
fi

BUNDLE_FILE="reports/incident-bundle-$INCIDENT_ID.json"
SCOPE_AUDIT_FILE="reports/scope-audit-$INCIDENT_ID.json"
mkdir -p reports

export INCIDENT_ID
export SERVICE
export ENVIRONMENT
export START_TIME
export END_TIME
export TIMEZONE
export SCOPE_FILE
export BUNDLE_FILE
export SCOPE_AUDIT_FILE

# Phase 2 â†’ Phase 1 Pipeline: Scope & Reduce â†’ Normalize & Validate
python3 - <<'PYTHON_PIPELINE'
import json
import os
import re
import subprocess
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any

# ============================================================================
# PHASE 2: INCIDENT SCOPING & EVIDENCE REDUCTION
# ============================================================================

def load_scope() -> Dict[str, Any]:
    """Load and validate incident scope object."""
    with open(os.environ["SCOPE_FILE"], "r") as f:
        scope = json.load(f)
    
    if "service" not in scope or "time_window" not in scope:
        raise SystemExit("âŒ Scope missing required fields")
    
    if "start" not in scope["time_window"] or "end" not in scope["time_window"]:
        raise SystemExit("âŒ time_window missing start or end")
    
    return scope

def parse_iso(ts: str) -> datetime:
    """Parse ISO timestamp to datetime (UTC)."""
    if ts.endswith("Z"):
        return datetime.fromisoformat(ts.replace("Z", "+00:00"))
    if re.search(r"[+-]\d{2}:\d{2}$", ts):
        return datetime.fromisoformat(ts)
    return datetime.fromisoformat(ts).replace(tzinfo=timezone.utc)

def to_utc_iso(dt: datetime) -> str:
    """Convert datetime to UTC ISO string."""
    return dt.astimezone(timezone.utc).replace(microsecond=0).isoformat().replace("+00:00", "Z")

# STEP 2.2: Deployment anchoring
def find_deployments(events: List[Dict], start: datetime, end: datetime) -> List[Dict]:
    """Find deployments in incident window."""
    deployments = []
    for ev in events:
        ts = parse_iso(ev["time"])
        if start <= ts <= end:
            deployments.append(ev)
    return deployments

# STEP 2.3: Commit narrowing
def get_commits_around_deployments(deployments: List[Dict], 
                                   before: int, after: int) -> List[Dict]:
    """Get commits around deployment times."""
    all_commits = []
    
    for deployment in deployments:
        deploy_time = parse_iso(deployment["time"])
        
        start_time = (deploy_time - timedelta(minutes=before)).isoformat()
        end_time = (deploy_time + timedelta(minutes=after)).isoformat()
        
        cmd = ["git", "log", f"--since={start_time}", f"--until={end_time}",
               "--pretty=format:%H|%an|%aI|%s"]
        try:
            output = subprocess.check_output(cmd, text=True).strip()
            if output:
                for line in output.splitlines():
                    parts = line.split("|", 3)
                    if len(parts) == 4:
                        commit_hash, author, timestamp, message = parts
                        commit = {
                            "commit_hash": commit_hash,
                            "author": author,
                            "timestamp": to_utc_iso(parse_iso(timestamp)),
                            "message": message,
                        }
                        if not any(c["commit_hash"] == commit_hash for c in all_commits):
                            all_commits.append(commit)
        except subprocess.CalledProcessError:
            pass
    
    return all_commits

# STEP 2.5: Log scoping
def filter_logs(lines: List[str], min_severity: str) -> tuple:
    """Filter logs by severity."""
    severity_order = {"INFO": 0, "WARN": 1, "ERROR": 2}
    min_level = severity_order.get(min_severity, 1)
    
    included = []
    excluded = 0
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        parts = line.split(" ", 3)
        if len(parts) < 4:
            excluded += 1
            continue
        
        severity = parts[1].strip()
        if severity not in severity_order:
            excluded += 1
            continue
        
        if severity_order[severity] < min_level:
            if severity == "INFO":
                message = parts[3] if len(parts) >= 4 else ""
                if not any(kw in message for kw in ["Starting", "Deployment", "Shutdown"]):
                    excluded += 1
                    continue
            else:
                excluded += 1
                continue
        
        included.append(line)
    
    return included, excluded

# STEP 2.6: Metric dimension scoping
def filter_metrics(metrics: Dict, include_dims: List[str]) -> tuple:
    """Filter metrics by dimension."""
    if not include_dims:
        return metrics, 0
    
    included = {}
    excluded = 0
    
    for key, value in metrics.items():
        if key == "timestamp" or key in include_dims:
            included[key] = value
        else:
            excluded += 1
    
    return included, excluded

# Execute Phase 2: Scoping & Reduction
scope = load_scope()
service = scope["service"]
start_time = parse_iso(scope["time_window"]["start"])
end_time = parse_iso(scope["time_window"]["end"])

commit_window = scope.get("commit_window", {"before": 20, "after": 5})
paths = scope.get("paths", [])
log_policy = scope.get("log_policy", {"min_severity": "WARN"})
metric_policy = scope.get("metric_policy", {"include": []})

# ============================================================================
# ADAPTER INVOCATION (produces raw events, not yet scoped or validated)
# ============================================================================

def invoke_evidence_adapter(filepath: str, source_type: str) -> Dict:
    """
    Route raw evidence through appropriate adapter to enforce contract format.
    
    Returns: Evidence object with events (NOT yet scoped or validated)
    """
    adapter_map = {
        "hadoop": "./adapters/hadoop-adapter.py",
        # Future: "elasticsearch": "./adapters/elasticsearch-adapter.py",
        # Future: "kubernetes": "./adapters/kubernetes-adapter.py",
    }
    
    adapter_path = adapter_map.get(source_type)
    if not adapter_path:
        raise ValueError(f"No adapter found for source type: {source_type}")
    
    if not os.path.exists(adapter_path):
        raise FileNotFoundError(f"Adapter not found: {adapter_path}")
    
    # Invoke adapter
    try:
        result = subprocess.check_output(
            ["python3", adapter_path, filepath],
            text=True,
            stderr=subprocess.PIPE
        )
        evidence = json.loads(result)
        return evidence
    except subprocess.CalledProcessError as e:
        raise SystemExit(f"âŒ Adapter failed: {e.stderr}")
    except json.JSONDecodeError as e:
        raise SystemExit(f"âŒ Adapter output invalid JSON: {e}")

# ============================================================================
# PHASE 1: EVIDENCE CONTRACT VALIDATION (happens AFTER Phase 2 scoping)
# ============================================================================

def validate_evidence_contract(evidence: Dict, source_type: str) -> tuple:
    """
    Validate evidence against strict contract format.
    
    Returns: (is_valid, violations)
    """
    violations = []
    
    # Check required top-level keys
    if "source" not in evidence:
        violations.append("Missing 'source' field")
    if "quality" not in evidence:
        violations.append("Missing 'quality' metadata")
    if "signals" not in evidence:
        violations.append("Missing 'signals' array")
    
    if violations:
        return False, violations
    
    # Validate quality metadata
    quality = evidence.get("quality", {})
    if "completeness" not in quality or quality["completeness"] not in ["COMPLETE", "PARTIAL", "INCOMPLETE"]:
        violations.append(f"Invalid completeness: {quality.get('completeness')}")
    if "confidence_penalty" not in quality or not isinstance(quality["confidence_penalty"], (int, float)):
        violations.append("Missing or invalid confidence_penalty")
    
    # Validate signals
    signals = evidence.get("signals", [])
    if not isinstance(signals, list):
        violations.append("Signals must be an array")
        return False, violations
    
    for idx, signal in enumerate(signals):
        # Check timestamp format (ISO-8601 UTC)
        # Signals can have either 'timestamp' or 'first_seen' (for aggregated events)
        timestamp_field = signal.get("timestamp") or signal.get("first_seen")
        if not timestamp_field:
            violations.append(f"Signal {idx}: missing timestamp/first_seen")
        else:
            if not re.match(r"^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z$", timestamp_field):
                violations.append(f"Signal {idx}: invalid timestamp format (expected ISO-8601 UTC): {timestamp_field}")
        
        # Check severity (only INFO, WARN, ERROR allowed)
        if "severity" not in signal:
            violations.append(f"Signal {idx}: missing severity")
        elif signal["severity"] not in ["INFO", "WARN", "ERROR"]:
            violations.append(f"Signal {idx}: invalid severity '{signal['severity']}' (allowed: INFO, WARN, ERROR)")
        
        # Check event type (must be generic, no vendor jargon)
        if "event" not in signal:
            violations.append(f"Signal {idx}: missing event type")
        else:
            event = signal["event"]
            # Reject vendor-specific terms
            forbidden_terms = ["hadoop", "datanode", "namenode", "spark", "kafka", "kubernetes", "k8s"]
            if any(term in event.lower() for term in forbidden_terms):
                violations.append(f"Signal {idx}: event type contains vendor jargon: '{event}'")
        
        # Check aggregation (signals must have count if aggregated)
        if "count" in signal and not isinstance(signal["count"], int):
            violations.append(f"Signal {idx}: count must be integer")
    
    return len(violations) == 0, violations

# Load raw data
with open("evidence/deployments.json", "r") as f:
    raw_deployments = json.load(f)
with open("evidence/metrics.json", "r") as f:
    raw_metrics = json.load(f)

# ============================================================================
# PHASE 2: INCIDENT SCOPING & EVIDENCE REDUCTION
# ============================================================================

def scope_events_by_time(events: List[Dict], start: datetime, end: datetime, buffer_minutes: int = 0) -> tuple:
    """Step 2.1: Time window filtering (PRIMARY CUT)"""
    included = []
    excluded = 0
    
    if buffer_minutes > 0:
        start = start - timedelta(minutes=buffer_minutes)
        end = end + timedelta(minutes=buffer_minutes)
    
    for event in events:
        timestamp_str = event.get("timestamp") or event.get("first_seen", "")
        if not timestamp_str:
            excluded += 1
            continue
        
        try:
            event_time = parse_iso(timestamp_str)
            if start <= event_time <= end:
                included.append(event)
            else:
                excluded += 1
        except:
            excluded += 1
    
    return included, excluded

def scope_events_by_severity(events: List[Dict], min_severity: str, allow_lifecycle: bool) -> tuple:
    """Step 2.2: Severity threshold filtering"""
    severity_order = {"INFO": 0, "WARN": 1, "ERROR": 2}
    min_level = severity_order.get(min_severity, 1)
    
    included = []
    excluded = 0
    
    lifecycle_events = ["service_start", "service_shutdown", "startup", "shutdown", "deployment"]
    
    for event in events:
        severity = event.get("severity", "INFO")
        event_type = event.get("event", "")
        
        if severity not in severity_order:
            excluded += 1
            continue
        
        # Allow lifecycle events even if below threshold
        if allow_lifecycle and event_type in lifecycle_events:
            included.append(event)
        elif severity_order[severity] >= min_level:
            included.append(event)
        else:
            excluded += 1
    
    return included, excluded

def scope_events_by_allowlist(events: List[Dict], allowlist: List[str]) -> tuple:
    """Step 2.3: Event allowlist filtering (KEY for Hadoop)"""
    if not allowlist:
        return events, 0
    
    included = []
    excluded = 0
    
    for event in events:
        event_type = event.get("event", "")
        if event_type in allowlist:
            included.append(event)
        else:
            excluded += 1
    
    return included, excluded

def scope_events_by_component(events: List[Dict], target_service: str, allowed_components: List[str]) -> tuple:
    """Step 2.4: Component relevance check"""
    included = []
    excluded = 0
    
    # If no explicit allowlist, just check against target service
    if not allowed_components:
        allowed_components = [target_service]
    
    for event in events:
        component = event.get("component", "")
        if component in allowed_components:
            included.append(event)
        else:
            excluded += 1
    
    return included, excluded

def deduplicate_events(events: List[Dict]) -> List[Dict]:
    """Step 2.5: Deduplication & consolidation"""
    # Group by (event_type, severity, component)
    groups = {}
    
    for event in events:
        key = (event.get("event"), event.get("severity"), event.get("component"))
        if key not in groups:
            groups[key] = []
        groups[key].append(event)
    
    deduplicated = []
    for key, group in groups.items():
        if len(group) == 1:
            deduplicated.append(group[0])
        else:
            # Merge multiple events
            total_count = sum(e.get("count", 1) for e in group)
            timestamps = []
            for e in group:
                if "first_seen" in e:
                    timestamps.append(e["first_seen"])
                if "last_seen" in e:
                    timestamps.append(e["last_seen"])
                if "timestamp" in e:
                    timestamps.append(e["timestamp"])
            
            timestamps.sort()
            merged = group[0].copy()
            merged["count"] = total_count
            if timestamps:
                merged["first_seen"] = timestamps[0]
                merged["last_seen"] = timestamps[-1]
                if "timestamp" in merged:
                    del merged["timestamp"]  # Use first_seen/last_seen for aggregated events
            
            deduplicated.append(merged)
    
    return deduplicated

# Check if we have raw Hadoop logs that need adapter processing
hadoop_log_path = "evidence/hadoop.log"
if os.path.exists(hadoop_log_path):
    print("ğŸ” Detected raw Hadoop logs - invoking evidence contract adapter")
    
    # Invoke Hadoop adapter (produces raw events, NOT validated yet)
    hadoop_evidence = invoke_evidence_adapter(hadoop_log_path, "hadoop")
    
    # ========================================================================
    # PHASE 2: SCOPE & REDUCE (happens BEFORE validation)
    # ========================================================================
    
    print("ğŸ“ Phase 2: Scoping & Reducing events")
    
    initial_count = len(hadoop_evidence.get("signals", []))
    scoped_events = hadoop_evidence.get("signals", [])
    
    exclusion_counts = {
        "outside_time_window": 0,
        "severity_below_threshold": 0,
        "event_not_allowlisted": 0,
        "component_mismatch": 0,
        "deduplicated": 0
    }
    
    # Step 2.1: Time window filtering
    scoped_events, excluded_time = scope_events_by_time(scoped_events, start_time, end_time, buffer_minutes=0)
    exclusion_counts["outside_time_window"] = excluded_time
    print(f"  Step 2.1 (Time): {initial_count} â†’ {len(scoped_events)} events (-{excluded_time})")
    
    # Step 2.2: Severity threshold filtering
    min_severity = log_policy.get("min_severity", "WARN")
    allow_lifecycle = log_policy.get("lifecycle_events", True)
    before_severity = len(scoped_events)
    scoped_events, excluded_severity = scope_events_by_severity(scoped_events, min_severity, allow_lifecycle)
    exclusion_counts["severity_below_threshold"] = excluded_severity
    print(f"  Step 2.2 (Severity): {before_severity} â†’ {len(scoped_events)} events (-{excluded_severity})")
    
    # Step 2.3: Event allowlist filtering
    event_allowlist = log_policy.get("event_allowlist", [])
    before_allowlist = len(scoped_events)
    scoped_events, excluded_allowlist = scope_events_by_allowlist(scoped_events, event_allowlist)
    exclusion_counts["event_not_allowlisted"] = excluded_allowlist
    print(f"  Step 2.3 (Allowlist): {before_allowlist} â†’ {len(scoped_events)} events (-{excluded_allowlist})")
    
    # Step 2.4: Component relevance check
    allowed_components = log_policy.get("include_components", [service])
    before_component = len(scoped_events)
    scoped_events, excluded_component = scope_events_by_component(scoped_events, service, allowed_components)
    exclusion_counts["component_mismatch"] = excluded_component
    print(f"  Step 2.4 (Component): {before_component} â†’ {len(scoped_events)} events (-{excluded_component})")
    
    # Step 2.5: Deduplication
    before_dedup = len(scoped_events)
    scoped_events = deduplicate_events(scoped_events)
    exclusion_counts["deduplicated"] = before_dedup - len(scoped_events)
    print(f"  Step 2.5 (Dedup): {before_dedup} â†’ {len(scoped_events)} events (-{exclusion_counts['deduplicated']})")
    
    # Failure mode: No events remaining
    if not scoped_events:
        raise SystemExit("âŒ Phase 2: No events remain after scoping. Aborting investigation.")
    
    # Failure mode: Only INFO remains (low signal)
    has_error_or_warn = any(e.get("severity") in ["ERROR", "WARN"] for e in scoped_events)
    if not has_error_or_warn:
        print("âš ï¸  Phase 2: Only INFO events remain (low signal)")
    
    # Generate scope audit
    scope_audit = {
        "source": hadoop_evidence.get("source"),
        "included": len(scoped_events),
        "excluded": initial_count - len(scoped_events),
        "exclusion_breakdown": exclusion_counts,
        "reduction_ratio": f"{(1 - len(scoped_events)/initial_count)*100:.1f}%"
    }
    
    print(f"âœ“ Phase 2 complete: {initial_count} events â†’ {len(scoped_events)} events (reduction: {scope_audit['reduction_ratio']})")
    
    # ========================================================================
    # PHASE 1: VALIDATE scoped events (happens AFTER reduction)
    # ========================================================================
    
    # Reconstruct evidence object with scoped events
    scoped_evidence = {
        "source": hadoop_evidence["source"],
        "quality": hadoop_evidence["quality"],
        "signals": scoped_events
    }
    
    # NOW validate the scoped evidence
    is_valid, violations = validate_evidence_contract(scoped_evidence, "hadoop")
    if not is_valid:
        print("âŒ Phase 1: Evidence contract validation FAILED:")
        for v in violations:
            print(f"   - {v}")
        raise SystemExit("âŒ Contract violations detected. Aborting investigation.")
    
    print(f"âœ“ Phase 1 complete: Evidence contract validated ({len(scoped_events)} scoped signals)")
    
    # Convert contract signals to log format for downstream processing
    raw_logs = []
    for signal in scoped_events:
        timestamp = signal.get("timestamp") or signal.get("first_seen", "")
        message = f"{signal['event']} (count: {signal.get('count', 1)})"
        if "context" in signal:
            message += f" - {signal['context']}"
        log_line = f"{timestamp} {signal['severity']} {hadoop_evidence['source']} {message}\n"
        raw_logs.append(log_line)
    
    # Store quality penalties for later propagation
    evidence_quality_penalties = [{
        "source": "hadoop_logs",
        "reason": "; ".join(hadoop_evidence['quality'].get('notes', [])),
        "penalty": -hadoop_evidence['quality']['confidence_penalty']
    }]
    
    # Store scope audit for reporting
    phase2_scope_audit = scope_audit
else:
    # Fallback to standard app.log
    with open("evidence/app.log", "r") as f:
        raw_logs = f.readlines()
    evidence_quality_penalties = []
    phase2_scope_audit = None

# STEP 2.2: Deployment anchoring
deployments = find_deployments(raw_deployments, start_time, end_time)
if not deployments:
    raise SystemExit("âŒ No deployment events in incident window. Aborting.")

print(f"âœ“ Found {len(deployments)} deployment(s) in incident window")

# STEP 2.3: Commit narrowing
commits = get_commits_around_deployments(
    deployments, 
    commit_window["before"], 
    commit_window["after"]
)
excluded_commits = 0
print(f"âœ“ Commit narrowing: {len(commits)} commits in window")

# STEP 2.5: Log scoping
filtered_logs, excluded_logs = filter_logs(
    raw_logs,
    log_policy.get("min_severity", "WARN")
)
print(f"âœ“ Log scoping: {len(filtered_logs)} logs included, {excluded_logs} excluded")

# STEP 2.6: Metric scoping
filtered_metrics, excluded_metrics = filter_metrics(
    raw_metrics,
    metric_policy.get("include", [])
)
print(f"âœ“ Metric scoping: {len(filtered_metrics)-1} dimensions included, {excluded_metrics} excluded")

# Scope audit
scope_audit = {
    "scope_summary": {
        "service": service,
        "time_window": f"{to_utc_iso(start_time)} to {to_utc_iso(end_time)}",
        "paths": paths if paths else "all",
        "commit_buffer": f"-{commit_window['before']}m / +{commit_window['after']}m",
    },
    "reduction_summary": {
        "commits": {"included": len(commits), "excluded": excluded_commits},
        "logs": {"included": len(filtered_logs), "excluded": excluded_logs},
        "metrics": {"included": len(filtered_metrics) - 1, "excluded": excluded_metrics},
    },
    "exclusion_reasons": [
        "outside_commit_window",
        "severity_below_threshold",
        "metric_dimension_not_in_scope",
    ],
}

# Add Phase 2 Hadoop event reduction audit if available
if phase2_scope_audit is not None:
    scope_audit["hadoop_event_reduction"] = phase2_scope_audit

# ============================================================================
# PHASE 1: NORMALIZATION & VALIDATION
# ============================================================================

def diff_summary(commit_hash: str):
    """Get semantic diff summary."""
    cmd = ["git", "show", commit_hash, "--name-status", "--pretty=format:"]
    try:
        output = subprocess.check_output(cmd, text=True)
    except subprocess.CalledProcessError:
        return []
    
    diffs = []
    for line in output.splitlines():
        if not line.strip():
            continue
        parts = line.split("\t", 1)
        if len(parts) != 2:
            continue
        change_type, path = parts
        
        semantic_hint = ""
        if path == "app.py":
            show = subprocess.check_output(["git", "show", commit_hash, "--", path], text=True)
            if "cache.append" in show:
                semantic_hint = "adds unbounded cache append"
            elif '\"status\": \"error\"' in show:
                semantic_hint = "adds lightweight input validation"
        
        diffs.append({
            "file_path": path,
            "change_type": change_type,
            "semantic_hint": semantic_hint or "n/a",
        })
    return diffs

def normalize_deployments(events: List[Dict]) -> List[Dict]:
    """Normalize deployment events."""
    normalized = []
    for ev in events:
        normalized.append({
            "timestamp": to_utc_iso(parse_iso(ev["time"])),
            "service": service,
            "version": ev.get("version"),
            "commit_hash": ev.get("commit"),
        })
    return normalized

def normalize_logs(lines: List[str]) -> List[Dict]:
    """Normalize log entries."""
    entries = []
    for line in lines:
        line = line.strip()
        if not line:
            continue
        parts = line.split(" ", 3)
        if len(parts) < 4:
            continue
        ts, severity, _, message = parts[0], parts[1], parts[2], parts[3]
        entries.append({
            "timestamp": to_utc_iso(parse_iso(ts)),
            "severity": severity.strip(),
            "component": service,
            "message": message.strip(),
        })
    return entries

def aggregate_metrics(metrics: Dict) -> Dict:
    """Aggregate metrics."""
    summary = {}
    for key, values in metrics.items():
        if key == "timestamp" or not values:
            continue
        baseline = values[0]
        pre_incident = values[1] if len(values) > 1 else values[0]
        peak = max(values)
        delta = peak - baseline
        summary[key] = {
            "baseline": baseline,
            "pre_incident": pre_incident,
            "peak": peak,
            "delta": delta,
            "unit": "mb" if "memory" in key else "pct",
            "direction": "up" if delta > 0 else "down" if delta < 0 else "flat",
        }
    return summary

# Normalize scoped data
deployment_events = normalize_deployments(deployments)
log_entries = normalize_logs(filtered_logs)
metric_summary = aggregate_metrics(filtered_metrics)

# Generate diffs
diffs = []
for c in commits:
    diffs.extend(diff_summary(c["commit_hash"]))

# Integrity accounting
missing_sources = []
confidence_penalties = evidence_quality_penalties.copy()  # Start with adapter penalties

if not log_entries:
    missing_sources.append("application_logs")
    confidence_penalties.append({"reason": "Missing or empty logs", "penalty": -10})

if not metric_summary:
    missing_sources.append("metrics")
    confidence_penalties.append({"reason": "Missing or empty metrics", "penalty": -15})

# Build final bundle
bundle = {
    "metadata": {
        "incident_id": os.environ["INCIDENT_ID"],
        "service": service,
        "environment": os.environ["ENVIRONMENT"],
        "start_time": to_utc_iso(start_time),
        "end_time": to_utc_iso(end_time),
        "timezone": os.environ["TIMEZONE"],
    },
    "version_control": {
        "commits": commits,
        "diffs": diffs,
    },
    "deployments": {
        "events": deployment_events,
    },
    "logs": {
        "entries": log_entries,
    },
    "metrics": {
        "aggregates": metric_summary,
    },
    "integrity": {
        "missing_sources": missing_sources,
        "confidence_penalties": confidence_penalties,
    },
}

# Save artifacts
with open(os.environ["BUNDLE_FILE"], "w") as f:
    json.dump(bundle, f, indent=2)

with open(os.environ["SCOPE_AUDIT_FILE"], "w") as f:
    json.dump(scope_audit, f, indent=2)

print(f"âœ“ Phase 2 complete: Evidence scoped and reduced")
print(f"âœ“ Phase 1 complete: Evidence normalized and validated")
PYTHON_PIPELINE

echo
echo "ğŸ“¦ Incident Evidence Bundle saved to $BUNDLE_FILE"
echo "ğŸ“Š Scope Audit saved to $SCOPE_AUDIT_FILE"
echo

# Run Copilot investigation
BUNDLE_JSON="$(cat "$BUNDLE_FILE")"
SCOPE_AUDIT_JSON="$(cat "$SCOPE_AUDIT_FILE")"

PROMPT="You are a senior Site Reliability Engineer performing a blameless post-mortem using hypothesis-based reasoning.

IMPORTANT: Output ONLY the markdown post-mortem document. Do not output tool invocations, permission errors, or diagnostic messages. Start immediately with the markdown header.

Core Rules:
- Base conclusions ONLY on the evidence provided
- Distinguish causation from correlation
- Use structured hypothesis evaluation BEFORE concluding
- Reason in generic systems terms (avoid vendor-specific jargon)
- If uncertain, state uncertainty and quantify it

Scope Summary:
$(python3 -c "import json; s=json.load(open('$SCOPE_AUDIT_FILE')); print('- Service: ' + s['scope_summary']['service']); print('- Time window: ' + s['scope_summary']['time_window']); print('- Paths: ' + str(s['scope_summary']['paths'])); print('- Commit buffer: ' + s['scope_summary']['commit_buffer'])")

Reduction Summary:
$(python3 -c "import json; s=json.load(open('$SCOPE_AUDIT_FILE')); r=s['reduction_summary']; print(f\"- Commits considered: {r['commits']['included']} (excluded {r['commits']['excluded']})\"); print(f\"- Logs considered: {r['logs']['included']} (excluded {r['logs']['excluded']})\"); print(f\"- Metrics considered: {r['metrics']['included']} (excluded {r['metrics']['excluded']})\")")

Evidence Quality Notes:
- All timestamps normalized to UTC
- Missing sources: $(python3 -c "import json; b=json.load(open('$BUNDLE_FILE')); m=b['integrity']['missing_sources']; print(', '.join(m) if m else 'none')")
- Confidence penalties: $(python3 -c "import json; b=json.load(open('$BUNDLE_FILE')); p=b['integrity']['confidence_penalties']; print('; '.join([f\"{x['reason']} ({x['penalty']}%)\" for x in p]) if p else 'none')")

Incident Evidence Bundle (normalized JSON):
$BUNDLE_JSON

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
PHASE 3: HYPOTHESIS-BASED REASONING PROTOCOL (MANDATORY)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

You MUST NOT jump to conclusions. Follow this exact reasoning sequence:

STEP 1: HYPOTHESIS GENERATION (3-5 hypotheses, diverse categories)

Generate 3-5 plausible hypotheses spanning DIFFERENT categories:
- Application (code bugs, unbounded growth, error handling)
- Resource (memory exhaustion, disk space, file descriptors)
- Infrastructure (hardware failure, network partition, node crash)
- Traffic (load spike, retry storm, cascading failure)
- Dependency (external service failure, library bug, runtime issue)

EVEN IF a hypothesis seems unlikely, it must be considered to prevent anchoring bias.

STEP 2: EVIDENCE SYMMETRY (FOR + AGAINST, always)

For EACH hypothesis, you MUST provide:
- **Evidence FOR:** What supports this explanation?
- **Evidence AGAINST:** What contradicts or weakens this explanation?

If you cannot find evidence AGAINST, the hypothesis is not testable - reject it.

STEP 3: CONFIDENCE BUDGETING (â‰¤100% total)

- Assign confidence to each hypothesis (0-100%)
- Total confidence across ALL hypotheses MUST NOT exceed 100%
- Account for evidence quality penalties in your confidence
- Unused confidence = explicit uncertainty

Example confidence distribution:
- Hypothesis A: 65%
- Hypothesis B: 10%
- Hypothesis C: 5%
- Hypothesis D: 5% (ruled out after analysis)
- Uncertainty: 15% (from quality penalties + unknown factors)

STEP 4: EXPLICIT ELIMINATION (before selection)

You MUST explicitly state which hypotheses were ruled out and WHY.
Do NOT skip weak hypotheses - elimination is how you prove rigor.

STEP 5: ROOT CAUSE SELECTION (last step)

ONLY after elimination, select the primary root cause.
Root cause = highest confidence hypothesis that survived scrutiny.

CRITICAL CONSTRAINTS:
âœ“ Reason in GENERIC systems terms (resource exhaustion, crash, allocation failure)
âœ— NEVER use vendor jargon (Hadoop, DataNode, HDFS, NameNode, Spark, Kubernetes)
âœ“ Supporting factors are NOT separate hypotheses (they amplify the root cause)
âœ“ Confidence scores reflect evidence quality (account for penalties)

Required Output Structure (STRICT - do not skip or reorder):

# Incident Post-Mortem: $INCIDENT_ID

## Timeline
[Minute-by-minute sequence of events]

## Hypotheses Considered

### Hypothesis 1: [Name] (Category: [Application/Infra/Traffic/Config/Dependency])
**Evidence FOR:**
- [Evidence item 1]
- [Evidence item 2]

**Evidence AGAINST:**
- [Counter-evidence item 1]
- [Counter-evidence item 2]

**Confidence:** [X]%
**Status:** [CONFIRMED/RULED_OUT/POSSIBLE]

[Repeat for all hypotheses]

## Evidence Evaluation
[Cross-hypothesis evidence analysis]

## Ruled-Out Hypotheses
[Explicitly list hypotheses that were eliminated and WHY]

## Primary Root Cause
[Selected hypothesis with justification based on elimination]
**Commit:** [hash] - [message]
**Causal Chain:** [Step-by-step explanation]

## Contributing Factors
[Amplifiers of the root cause, NOT alternative explanations]

## Detection & Prevention Gaps
[What signals were missing or too late]

## Remediation & Follow-ups
[Concrete actions]

## Remaining Uncertainty
[What you don't know and why]

## Confidence Summary
- Primary root cause confidence: [X]%
- Total hypothesis confidence budget used: [Y]%
- Uncertainty factors: [list]

CRITICAL REQUIREMENTS:
- At least 3 hypotheses with different categories
- Each hypothesis must have BOTH evidence for AND against
- At least one hypothesis must be explicitly RULED_OUT
- Confidence scores must sum to â‰¤100%
- Contributing factors must NOT be listed as separate hypotheses
- Root cause selection must reference hypothesis elimination"

OUTPUT="reports/postmortem-$INCIDENT_ID.md"
PROMPT_FILE="reports/copilot-prompt-$INCIDENT_ID.txt"

# Save prompt
cat <<EOF > "$PROMPT_FILE"
$PROMPT
EOF

echo "ğŸ§  Copilot prompt saved to $PROMPT_FILE"
echo
echo "ğŸ¤– Invoking GitHub Copilot CLI for incident correlation and RCA"
echo "    â€¢ Inputs: scoped evidence (commits, logs, metrics)"
echo "    â€¢ Role: reasoning engine (timeline + root cause + remediation)"
echo

gh copilot -- -p "$PROMPT" | tee "$OUTPUT"

echo
echo "âœ… GitHub Copilot CLI analysis complete"
echo

# Phase 3: Validate hypothesis-based reasoning structure
echo "ğŸ” Phase 3: Validating hypothesis-based reasoning structure"
python3 - "$OUTPUT" <<'VALIDATE_HYPOTHESES'
import sys
import re

# Read the post-mortem
output_file = sys.argv[1]
with open(output_file, 'r') as f:
    content = f.read()

warnings = []
errors = []

# Check 1: Hypotheses Considered section exists
if '## Hypotheses Considered' not in content:
    errors.append("Missing '## Hypotheses Considered' section")
else:
    # Count hypotheses
    hypothesis_pattern = r'### Hypothesis \d+:'
    hypotheses = re.findall(hypothesis_pattern, content)
    hyp_count = len(hypotheses)
    
    if hyp_count < 3:
        errors.append(f"Only {hyp_count} hypotheses found (minimum: 3)")
    elif hyp_count > 5:
        warnings.append(f"Found {hyp_count} hypotheses (recommended: 3-5)")
    else:
        print(f"   âœ“ Found {hyp_count} hypotheses")
    
    # Check for category diversity
    category_pattern = r'\(Category: (Application|Resource|Infrastructure|Traffic|Dependency)\)'
    categories = re.findall(category_pattern, content)
    unique_categories = set(categories)
    
    if len(unique_categories) < 2:
        warnings.append(f"Low category diversity: only {len(unique_categories)} unique categories (recommended: 3+)")
    else:
        print(f"   âœ“ Category diversity: {len(unique_categories)} distinct categories ({', '.join(unique_categories)})")
    
    # Check for evidence symmetry (FOR and AGAINST)
    for_count = content.count('**Evidence FOR:**')
    against_count = content.count('**Evidence AGAINST:**')
    
    if for_count != hyp_count or against_count != hyp_count:
        errors.append(f"Evidence asymmetry detected: {for_count} FOR, {against_count} AGAINST (expected {hyp_count} each)")
    else:
        print(f"   âœ“ Evidence symmetry maintained ({hyp_count} FOR + {hyp_count} AGAINST)")
    
    # Check for confidence scores
    confidence_pattern = r'\*\*Confidence:\*\* (\d+)%'
    confidences = [int(x) for x in re.findall(confidence_pattern, content)]
    
    if confidences:
        total_confidence = sum(confidences)
        if total_confidence > 100:
            errors.append(f"Confidence budget exceeded: {total_confidence}% (maximum: 100%)")
        else:
            remaining = 100 - total_confidence
            print(f"   âœ“ Confidence budget: {total_confidence}% used, {remaining}% uncertainty")
    else:
        warnings.append("No confidence scores found")
    
    # Check for status markers
    status_count = content.count('**Status:**')
    if status_count != hyp_count:
        warnings.append(f"Missing status markers: found {status_count}, expected {hyp_count}")
    else:
        print(f"   âœ“ All hypotheses have status markers")

# Check 2: Ruled-Out Hypotheses section
if '## Ruled-Out Hypotheses' not in content:
    errors.append("Missing '## Ruled-Out Hypotheses' section")
else:
    ruled_out = content.count('RULED_OUT')
    if ruled_out == 0:
        warnings.append("No hypotheses explicitly ruled out")
    else:
        print(f"   âœ“ {ruled_out} hypothesis(es) explicitly ruled out")

# Check 3: No vendor jargon (generic systems reasoning)
vendor_terms = ['hadoop', 'hdfs', 'datanode', 'namenode', 'spark', 'kafka', 'kubernetes', 'k8s']
content_lower = content.lower()
found_jargon = [term for term in vendor_terms if term in content_lower]

if found_jargon:
    warnings.append(f"Vendor jargon detected: {', '.join(found_jargon)} - prefer generic systems terms")
else:
    print(f"   âœ“ Generic systems reasoning (no vendor jargon)")

# Check 4: Required sections present
required_sections = [
    '## Timeline',
    '## Evidence Evaluation',
    '## Primary Root Cause',
    '## Remaining Uncertainty',
    '## Confidence Summary'
]

missing_sections = [s for s in required_sections if s not in content]
if missing_sections:
    errors.append(f"Missing required sections: {', '.join(missing_sections)}")
else:
    print(f"   âœ“ All required sections present")

# Report
if errors:
    print("\nâš ï¸  Validation ERRORS:")
    for e in errors:
        print(f"   âœ— {e}")
    sys.exit(1)

if warnings:
    print("\nâš ï¸  Validation WARNINGS:")
    for w in warnings:
        print(f"   â€¢ {w}")

if not errors and not warnings:
    print("\nâœ… Hypothesis validation passed")
VALIDATE_HYPOTHESES

PHASE3_VALID=$?

if [ $PHASE3_VALID -eq 0 ]; then
    echo
    echo "ğŸ“„ AI-generated post-mortem: $OUTPUT"
    echo "   â€¢ Phase 3 reasoning structure validated"
else
    echo
    echo "âš ï¸  Post-mortem generated with validation warnings/errors"
    echo "ğŸ“„ Output: $OUTPUT"
fi

echo
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ“‹ Phase 4: Human Review & Decision Accountability"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo

# Extract AI proposal summary from post-mortem
python3 - "$OUTPUT" <<'EXTRACT_SUMMARY' > /tmp/sherlock-summary-$$.json
import sys
import re
import json

with open(sys.argv[1], 'r') as f:
    content = f.read()

# Extract primary root cause - look for the hypothesis name
primary_match = re.search(r'### Hypothesis 1: (.+?) \(Category:', content)
if not primary_match:
    primary_match = re.search(r'## Primary Root Cause\s+\*\*(.+?)\*\*', content, re.DOTALL)
    if not primary_match:
        primary_match = re.search(r'## Primary Root Cause\s+(.+?)(?:\n\n|\*\*)', content, re.DOTALL)
primary_cause = primary_match.group(1).strip() if primary_match else "Unknown"
# Limit length
primary_cause = primary_cause[:150]

# Extract confidence - try multiple patterns
confidence_match = re.search(r'\*\*Primary root cause confidence:\*\* (\d+)%', content)
if not confidence_match:
    confidence_match = re.search(r'Primary root cause confidence:\s*(\d+)%', content)
if not confidence_match:
    confidence_match = re.search(r'\*\*Confidence:\*\* (\d+)%.*?CONFIRMED', content, re.DOTALL)
confidence = int(confidence_match.group(1)) if confidence_match else 0

# Extract ruled out count
ruled_out_count = len(re.findall(r'\*\*Status:\*\* RULED_OUT', content))

# Extract remaining uncertainty
uncertainty_match = re.search(r'Total hypothesis confidence budget used:\s*(\d+)%', content)
if uncertainty_match:
    uncertainty = 100 - int(uncertainty_match.group(1))
else:
    uncertainty = 100 - confidence

result = {
    "primary_cause": primary_cause,
    "confidence": confidence,
    "ruled_out_count": ruled_out_count,
    "uncertainty": uncertainty
}

print(json.dumps(result))
EXTRACT_SUMMARY

# Parse JSON and export variables
PRIMARY_CAUSE=$(python3 -c "import json; d=json.load(open('/tmp/sherlock-summary-$$.json')); print(d['primary_cause'])")
CONFIDENCE=$(python3 -c "import json; d=json.load(open('/tmp/sherlock-summary-$$.json')); print(d['confidence'])")
RULED_OUT_COUNT=$(python3 -c "import json; d=json.load(open('/tmp/sherlock-summary-$$.json')); print(d['ruled_out_count'])")
UNCERTAINTY=$(python3 -c "import json; d=json.load(open('/tmp/sherlock-summary-$$.json')); print(d['uncertainty'])")
rm -f /tmp/sherlock-summary-$$.json

# Present review summary
echo "Incident: $INCIDENT_ID"
echo "Service: $SERVICE"
echo
echo "AI-Proposed Root Cause ($CONFIDENCE% confidence):"
echo "  â€¢ $PRIMARY_CAUSE"
echo
echo "Analysis Quality:"
echo "  â€¢ Hypotheses evaluated: $((RULED_OUT_COUNT + 1))"
echo "  â€¢ Hypotheses ruled out: $RULED_OUT_COUNT"
echo "  â€¢ Remaining uncertainty: ${UNCERTAINTY}%"
echo
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo
echo "Choose a decision:"
echo
echo "  [A] ACCEPT"
echo "      â€¢ Keep AI analysis as-is (no changes)"
echo "      â€¢ Accountability: Human reviewer endorses AI conclusion"
echo "      â€¢ Triggers: finalize review record, write to institutional memory"
echo
echo "  [M] MODIFY"
echo "      â€¢ AI analysis is directionally correct but requires adjustment"
echo "      â€¢ Override root cause, confidence, or remediation"
echo "      â€¢ AI proposal preserved for audit trail"
echo
echo "  [R] REJECT"
echo "      â€¢ Analysis is not actionable - insufficient evidence or contradictory signals"
echo "      â€¢ AI output preserved (not deleted)"
echo "      â€¢ Reason documented"
echo
read -p "Decision [A/M/R]: " DECISION

# Normalize to uppercase
DECISION=$(echo "$DECISION" | tr '[:lower:]' '[:upper:]')

# Validate decision
if [[ ! "$DECISION" =~ ^[AMR]$ ]]; then
    echo "âŒ Invalid decision. Must be A, M, or R."
    echo "   Review record marked as DRAFT"
    DECISION="DRAFT"
fi

echo
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Capture reviewer info
read -p "Reviewer name: " REVIEWER_NAME
read -p "Reviewer role [Incident Commander/SRE/Maintainer]: " REVIEWER_ROLE
read -p "Reviewer identifier (email/username): " REVIEWER_ID

DECISION_LABEL="DRAFT"
FINAL_ROOT_CAUSE="$PRIMARY_CAUSE"
FINAL_CONFIDENCE="$CONFIDENCE"
OVERRIDES_JSON="[]"
NOTES_JSON="[]"

case "$DECISION" in
    A)
        DECISION_LABEL="ACCEPTED"
        echo
        echo "âœ“ Decision: ACCEPTED"
        echo "  â€¢ AI analysis accepted without modification"
        echo "  â€¢ Accountability: Human reviewer endorses AI conclusion"
        ;;
    M)
        DECISION_LABEL="MODIFIED"
        echo
        echo "ğŸ“ Decision: MODIFIED"
        echo "   AI analysis is directionally correct but requires adjustment"
        echo "   Please specify overrides..."
        echo
        
        # Collect overrides
        OVERRIDES_JSON="[]"
        
        read -p "Modify root cause? [y/N]: " MODIFY_CAUSE
        if [[ "$MODIFY_CAUSE" =~ ^[Yy]$ ]]; then
            read -p "New root cause description: " NEW_CAUSE
            read -p "Rationale for change: " CAUSE_RATIONALE
            FINAL_ROOT_CAUSE="$NEW_CAUSE"
            
            OVERRIDES_JSON=$(python3 -c "import json; overrides = json.loads('$OVERRIDES_JSON'); overrides.append({'field': 'root_cause', 'original_value': '''$PRIMARY_CAUSE''', 'new_value': '''$NEW_CAUSE''', 'rationale': '''$CAUSE_RATIONALE'''}); print(json.dumps(overrides))")
        fi
        
        read -p "Modify confidence? [y/N]: " MODIFY_CONF
        if [[ "$MODIFY_CONF" =~ ^[Yy]$ ]]; then
            read -p "New confidence (0-100): " NEW_CONFIDENCE
            read -p "Rationale for change: " CONF_RATIONALE
            FINAL_CONFIDENCE="$NEW_CONFIDENCE"
            
            OVERRIDES_JSON=$(python3 -c "import json; overrides = json.loads('$OVERRIDES_JSON'); overrides.append({'field': 'confidence', 'original_value': $CONFIDENCE, 'new_value': $NEW_CONFIDENCE, 'rationale': '''$CONF_RATIONALE'''}); print(json.dumps(overrides))")
        fi
        
        read -p "Additional notes (e.g., context-specific concerns): " NOTES
        if [ -n "$NOTES" ]; then
            NOTES_JSON=$(python3 -c "import json; print(json.dumps(['$NOTES']))")
        fi
        
        echo
        echo "âœ“ Modifications recorded with rationale"
        echo "  â€¢ AI proposal preserved for audit trail"
        echo "  â€¢ Human overrides documented"
        ;;
    R)
        DECISION_LABEL="REJECTED"
        echo
        echo "âš ï¸  Decision: REJECTED"
        echo "   Analysis is not actionable - insufficient evidence or contradictory signals"
        echo
        read -p "Primary reason for rejection: " REJECTION_REASON
        NOTES_JSON=$(python3 -c "import json; print(json.dumps(['REJECTED: $REJECTION_REASON', 'Analysis preserved for transparency but not endorsed']))")
        FINAL_ROOT_CAUSE="Analysis rejected - see notes"
        FINAL_CONFIDENCE="0"
        
        echo
        echo "âœ“ Rejection recorded"
        echo "  â€¢ Confidence set to 0%"
        echo "  â€¢ AI output preserved (not deleted)"
        echo "  â€¢ Reason documented"
        ;;
esac

# Ask for finalization
echo
read -p "Finalize this review? [Y/n]: " FINALIZE
APPROVAL_STATUS="DRAFT"
if [[ ! "$FINALIZE" =~ ^[Nn]$ ]]; then
    APPROVAL_STATUS="FINALIZED"
fi

# Generate Review Record
REVIEW_RECORD="reports/review-record-$INCIDENT_ID.yaml"
REVIEW_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

echo
echo "ğŸ’¾ Generating Review Record..."

python3 - "$REVIEW_RECORD" <<GENERATE_RR
import sys
import json
from datetime import datetime

output_file = sys.argv[1]

# Parse environment variables
incident_id = "$INCIDENT_ID"
reviewer_name = "$REVIEWER_NAME"
reviewer_role = "$REVIEWER_ROLE"
reviewer_id = "$REVIEWER_ID"
review_time = "$REVIEW_TIME"
primary_cause = "$PRIMARY_CAUSE"
confidence = "$CONFIDENCE"
ruled_out_count = "$RULED_OUT_COUNT"
decision = "$DECISION_LABEL"
final_cause = "$FINAL_ROOT_CAUSE"
final_confidence = "$FINAL_CONFIDENCE"
overrides_json = '''$OVERRIDES_JSON'''
notes_json = '''$NOTES_JSON'''
approval_status = "$APPROVAL_STATUS"

# Parse JSON
overrides = json.loads(overrides_json)
notes = json.loads(notes_json)

# Build Review Record
review_record = f"""# Review Record: {incident_id}
# Generated by Sherlock Phase 4: Human Review & Decision Accountability
# Timestamp: {review_time}

incident_id: {incident_id}

reviewer:
  name: "{reviewer_name}"
  role: "{reviewer_role}"  # Incident Commander | SRE | Maintainer
  identifier: "{reviewer_id}"

review_time: {review_time}

# AI Proposal (Phase 3 Output)
ai_proposal:
  primary_root_cause: "{primary_cause}"
  confidence: {confidence}  # 0-100%
  ruled_out_count: {ruled_out_count}

# Human Decision (Phase 4)
human_decision:
  decision: {decision}  # ACCEPTED | MODIFIED | REJECTED
  final_root_cause: "{final_cause}"
  final_confidence: {final_confidence}  # 0-100%

# Overrides (if decision = MODIFIED)
overrides:
"""

# Add overrides
if overrides:
    for override in overrides:
        review_record += f'''  - field: {override['field']}
    original_value: "{override['original_value']}"
    new_value: "{override['new_value']}"
    rationale: "{override['rationale']}"
'''
else:
    review_record += '  []\n'

review_record += f"\nnotes:\n"

# Add notes
if notes:
    for note in notes:
        review_record += f'  - "{note}"\n'
else:
    review_record += '  []\n'

review_record += f"""
# Approval Status
approval:
  status: {approval_status}  # FINALIZED | DRAFT

# Artifact References
artifacts:
  ai_postmortem: reports/postmortem-{incident_id}.md
  evidence_bundle: reports/incident-bundle-{incident_id}.json
  scope_audit: reports/scope-audit-{incident_id}.json
  copilot_prompt: reports/copilot-prompt-{incident_id}.txt

# Governance Notes
# - AI proposes, humans decide, Sherlock records
# - This record is immutable once finalized
# - Overrides preserve AI proposal for audit trail
# - REJECTED analyses are preserved for transparency
"""

# Write to file
with open(output_file, 'w') as f:
    f.write(review_record)

print(f"âœ“ Review record written: {output_file}")
GENERATE_RR

echo
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ… Phase 4 Complete"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo
echo "ğŸ“Š Decision Summary:"
echo "   â€¢ Decision: $DECISION_LABEL"
echo "   â€¢ Final Root Cause: $FINAL_ROOT_CAUSE"
echo "   â€¢ Final Confidence: ${FINAL_CONFIDENCE}%"
echo "   â€¢ Approval Status: $APPROVAL_STATUS"
echo
echo "ğŸ“ Artifacts:"
echo "   â€¢ AI Post-Mortem: $OUTPUT"
echo "   â€¢ Review Record: $REVIEW_RECORD"
echo
if [ "$APPROVAL_STATUS" = "FINALIZED" ]; then
    echo "âœ“ Investigation complete and finalized"
    
    # Phase 5: Write to Institutional Memory (only if finalized)
    echo
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ğŸ’¾ Phase 5: Writing to Organizational Memory"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo
    
    INCIDENT_STORE="incidents"
    mkdir -p "$INCIDENT_STORE"
    
    INCIDENT_FILE="$INCIDENT_STORE/$INCIDENT_ID.yaml"
    
    # Check for duplicate (append-only protection)
    if [ -f "$INCIDENT_FILE" ]; then
        echo "âš ï¸  Incident $INCIDENT_ID already exists in memory"
        echo "   File: $INCIDENT_FILE"
        echo "   Phase 5 write aborted (append-only guarantee)"
    else
        # Extract incident index record
        python3 - "$REVIEW_RECORD" "$OUTPUT" "$SCOPE_AUDIT_FILE" "$INCIDENT_FILE" <<'EXTRACT_INDEX'
import sys
import json
import re
from datetime import datetime

review_record_path = sys.argv[1]
postmortem_path = sys.argv[2]
scope_audit_path = sys.argv[3]
output_path = sys.argv[4]

# Load review record (YAML format - parse manually to avoid dependency)
with open(review_record_path, 'r') as f:
    lines = f.readlines()

def parse_yaml_value(lines, key):
    """Simple YAML parser for our specific format"""
    for i, line in enumerate(lines):
        if line.strip().startswith(f'{key}:'):
            value = line.split(':', 1)[1].strip().strip('"')
            return value
    return None

def parse_nested_yaml_value(lines, parent_key, child_key):
    """Parse nested YAML values"""
    in_section = False
    for line in lines:
        if line.strip().startswith(f'{parent_key}:'):
            in_section = True
            continue
        if in_section:
            if line.startswith('  ') and child_key in line:
                value = line.split(':', 1)[1].strip().strip('"')
                return value
            if not line.startswith('  '):
                break
    return None

incident_id = parse_yaml_value(lines, 'incident_id')
review_time = parse_yaml_value(lines, 'review_time')
reviewer_role = parse_nested_yaml_value(lines, 'reviewer', 'role')
ai_confidence = parse_nested_yaml_value(lines, 'ai_proposal', 'confidence')
final_confidence = parse_nested_yaml_value(lines, 'human_decision', 'final_confidence')
final_root_cause = parse_nested_yaml_value(lines, 'human_decision', 'final_root_cause')
decision_type = parse_nested_yaml_value(lines, 'human_decision', 'decision')

# Load scope audit
with open(scope_audit_path, 'r') as f:
    scope_audit = json.load(f)

service = scope_audit['scope_summary']['service']

# Load postmortem for signal extraction
with open(postmortem_path, 'r') as f:
    postmortem = f.read()

# Extract signals from postmortem
signals = []
if 'memory' in postmortem.lower() and 'growth' in postmortem.lower():
    signals.append('memory_growth')
if 'error rate' in postmortem.lower() or 'error_rate' in postmortem.lower():
    signals.append('error_rate_spike')
if 'latency' in postmortem.lower() or 'timeout' in postmortem.lower():
    signals.append('latency_degradation')
if 'crash' in postmortem.lower():
    signals.append('crash_loop')

# Extract category from root cause
category = "Application"  # Default
if 'cache' in final_root_cause.lower() or 'code' in final_root_cause.lower():
    category = "Application"
elif 'config' in final_root_cause.lower() or 'deployment' in final_root_cause.lower():
    category = "Config"
elif 'infra' in final_root_cause.lower() or 'hardware' in final_root_cause.lower():
    category = "Infra"
elif 'dependency' in final_root_cause.lower() or 'library' in final_root_cause.lower():
    category = "Dependency"
elif 'traffic' in final_root_cause.lower() or 'load' in final_root_cause.lower():
    category = "Traffic"

# Count hypotheses from postmortem
hypothesis_count = len(re.findall(r'### Hypothesis \d+:', postmortem))
ruled_out_count = len(re.findall(r'\*\*Status:\*\* RULED_OUT', postmortem))

# Extract remediation promises from postmortem
remediation_section = ""
if '## Remediation' in postmortem:
    remediation_section = postmortem.split('## Remediation')[1].split('##')[0]

promised_actions = []
for line in remediation_section.split('\n'):
    if line.strip().startswith('-') or re.match(r'^\d+\.', line.strip()):
        action = re.sub(r'^[-\d\.]+\s*', '', line.strip())
        if action and len(action) > 10:  # Filter out short lines
            promised_actions.append(action[:100])  # Limit length

ai_conf = int(ai_confidence) if ai_confidence else 0
human_conf = int(final_confidence) if final_confidence else 0
delta = human_conf - ai_conf

# Build incident index record
incident_record = f"""# Incident Index Record: {incident_id}

incident_id: {incident_id}
timestamp: {review_time}

service: {service}
environment: demo

final_root_cause:
  summary: "{final_root_cause}"
  category: {category}

decision:
  type: {decision_type}
  reviewer_role: {reviewer_role}
  final_confidence: {human_conf}

ai_vs_human:
  ai_confidence: {ai_conf}
  human_confidence: {human_conf}
  delta: {delta}

signals:
"""

for signal in signals:
    incident_record += f"  - {signal}\n"

if not signals:
    incident_record += "  []\n"

incident_record += f"""
hypotheses:
  total: {hypothesis_count}
  ruled_out: {ruled_out_count}

remediation:
  promised:
"""

for action in promised_actions[:5]:  # Limit to 5 actions
    incident_record += f'    - "{action}"\n'

if not promised_actions:
    incident_record += "    []\n"

incident_record += f"""  status:
"""

for action in promised_actions[:5]:
    incident_record += f'''    - action: "{action}"
      completed: false
'''

if not promised_actions:
    incident_record += "    []\n"

incident_record += f"""
artifacts:
  review_record: {review_record_path}
  postmortem: {postmortem_path}
"""

# Write to institutional memory
with open(output_path, 'w') as f:
    f.write(incident_record)

print(f"âœ“ Incident indexed: {output_path}")
print(f"  â€¢ Category: {category}")
print(f"  â€¢ Signals: {', '.join(signals) if signals else 'none'}")
print(f"  â€¢ Hypotheses: {hypothesis_count} total, {ruled_out_count} ruled out")
print(f"  â€¢ Confidence delta: {delta:+d}%")
EXTRACT_INDEX
        
        if [ $? -eq 0 ]; then
            echo
            echo "âœ… Organizational memory updated"
            echo "   â€¢ Append-only guarantee preserved"
            echo "   â€¢ No influence on future reasoning"
            echo "   â€¢ Query with: ./sherlock history"
            
            # Phase 6: Operational Integration (optional, read-only)
            if [ -f "phase6/phase6.sh" ]; then
                echo
                bash phase6/phase6.sh "$INCIDENT_ID"
            fi
            
            # Phase 7: Trust, Assurance & Verifiability (optional, observational)
            if [ -f "phase7/phase7.sh" ]; then
                echo
                bash phase7/phase7.sh "$INCIDENT_ID"
            fi
            
            # Phase 8: Lifecycle Summary (judge-visible timeline)
            echo
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo "Sherlock Incident Lifecycle Complete: $INCIDENT_ID"
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo
            echo "âœ“ Phase 1: Evidence validated & normalized"
            echo "âœ“ Phase 2: Scope reduced & focused"
            echo "âœ“ Phase 3: Hypotheses evaluated & confidence scored"
            echo "âœ“ Phase 4: Human decision recorded & governance enforced"
            echo "âœ“ Phase 5: Incident indexed in organizational memory"
            if [ -f "phase6/phase6.sh" ]; then
                echo "âœ“ Phase 6: Operational actions dispatched"
            fi
            if [ -f "phase7/phase7.sh" ]; then
                echo "âœ“ Phase 7: Trust artifacts generated & cryptographically bound"
            fi
            echo
            echo "All artifacts:"
            echo "  â€¢ Evidence: reports/incident-bundle-$INCIDENT_ID.json"
            echo "  â€¢ Scope: reports/scope-audit-$INCIDENT_ID.json"
            echo "  â€¢ Analysis: reports/post-mortem-$INCIDENT_ID.md"
            echo "  â€¢ Governance: reports/review-record-$INCIDENT_ID.yaml"
            echo "  â€¢ Memory: incidents/$INCIDENT_ID.yaml"
            if [ -f "phase7/provenance-$INCIDENT_ID.json" ]; then
                echo "  â€¢ Provenance: phase7/provenance-$INCIDENT_ID.json"
            fi
            if [ -f "phase7/trust-report-$INCIDENT_ID.md" ]; then
                echo "  â€¢ Trust Report: phase7/trust-report-$INCIDENT_ID.md"
            fi
            echo
            echo "Note: AI proposed. Human decided. System remembered. Actions executed."
            echo "      Every decision is auditable, immutable, and externally verifiable."
            echo
        else
            echo "âŒ Phase 5 write failed"
        fi
    fi
else
    echo "âš ï¸  Review marked as DRAFT - finalization required"
    echo "   Phase 5 skipped (institutional memory requires finalization)"
fi
